# Goal: Assess the candidateâ€™s ability to design and implement a data pipeline using Firebase as the source system. The candidate must use their own recipe as the initial seed data.

#  Recipe ETL & Analytics Pipeline  
A complete Firestore â†’ ETL â†’ Validation â†’ Analytics project

---

##  Project Overview

This project demonstrates an end-to-end **data engineering pipeline** using:

- **Google Firestore** (as the source database)
- **Python ETL scripts**
- **CSV-based normalized schema**
- **Data quality validation**
- **Analytics & insights generation**

The dataset simulates a recipe platform where users view, like, and interact with recipes.  
The pipeline extracts data, cleans it, transforms it into structured tables, validates fields, and produces insights.

---

#  Project Folder Structure
recipe-module/
â”‚
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ exports/ # JSON exported from Firestore
â”‚ â”‚ â”œâ”€â”€ recipes.json
â”‚ â”‚ â”œâ”€â”€ users.json
â”‚ â”‚ â””â”€â”€ interactions.json
â”‚ â”œâ”€â”€ csv_output/ # Normalized CSV files generated by ETL
â”‚ â”‚ â”œâ”€â”€ recipes.csv
â”‚ â”‚ â”œâ”€â”€ ingredients.csv
â”‚ â”‚ â”œâ”€â”€ steps.csv
â”‚ â”‚ â””â”€â”€ interactions.csv
â”‚ â”œâ”€â”€ validation_report.json # Output of validator
â”‚ â””â”€â”€ analytics_report.txt # Output of analytics
â”‚
â”œâ”€â”€ scripts/
â”‚ â”œâ”€â”€ export_firestore.py # Firestore â†’ JSON
â”‚ â”œâ”€â”€ etl_transform.py # JSON â†’ Normalized CSVs
â”‚ â”œâ”€â”€ validator.py # Data quality checks
â”‚ â””â”€â”€ analytics.py # Insights calculation
â”‚
â”œâ”€â”€ serviceAccountKey.json # Firebase Admin credentials
â””â”€â”€ README.md # (This file)


---

#  1. Data Model Explanation

The Firestore database contains **three main collections**.

---

## **1. `recipes` Collection**
Each recipe document contains:

| Field | Type | Description |
|-------|------|-------------|
| id | string | Document ID |
| title | string | Recipe name |
| description | string | Description |
| servings | int | Number of servings |
| prep_minutes | int | Preparation time |
| cook_minutes | int | Cooking time |
| total_minutes | int | Preparation time + Cooking time =Total time |
| difficulty | string | {easy, medium, hard} |
| ingredients | array<map> | [{name, quantity}] |
| steps | array<map> | [{order, text}] |
| created_at | timestamp | Date created |

---

## **2. `users` Collection**
| Field | Type |
|-------|------|
| id | string | Document ID |
| name | string |User name |
| email | string |User EmailId |
| created_at | timestamp | Date created |

---

## **3. `interactions` Collection**
Represents user engagement.

| Field | Type | Description |
|-------|------|-------------|
| id | string | Interaction ID |
| user_id | string | Reference to user |
| recipe_id | string | Reference to recipe |
| like | int | Number of like |
| views | int | Number of like |
| rating | int | Number of rating(1-5) |
| timestamp | timestamp | When interaction happened |

---

#  2. How to Run the Full Pipeline

Below are step-by-step instructions for running the entire pipeline from Firestore â†’ Analytics.

---

## âœ” Step 1 â€” Install dependencies
pip install pandas firebase-admin
If you are using a virtual environment, activate it first:
venv\Scripts\activate

---

## âœ” Step 2 â€” Export Firestore to JSON
python scripts/export_firestore.py
This generates:
data/exports/
recipes.json
users.json
interactions.json

---

## âœ” Step 3 â€” Run ETL (Transform JSON â†’ CSV)
python scripts/etl_transform.py
This produces normalized tables:
data/csv_output/
recipes.csv
ingredients.csv
steps.csv
interactions.csv

---

## âœ” Step 4 â€” Validate Data Quality
python scripts/validator.py
Creates report:
data/validation_report.json
This includes:
- valid records  
- invalid records  
- reasons for failures  

---

## âœ” Step 5 â€” Run analytics
python scripts/analytics.py
This generates:
data/analytics_report.txt

---

#  3. ETL Process Overview

The ETL pipeline runs in three phases:

---

## **Extract**
- Using Firebase Admin SDK  
- Data exported from Firestore collections  
- JSON files saved in `data/exports/`  
- Timestamp fields converted to ISO strings  

---

## **Transform**
Handled by `etl_transform.py`:

- Nested Firestore structures are normalized  
- Data split into relational CSVs  
- Derived fields computed (e.g., total_minutes)  
- Ensures clean schema for analytics  

Tables produced:

| CSV File | Description |
|----------|-------------|
| recipes.csv | Main recipe attributes |
| ingredients.csv | Each ingredient per row |
| steps.csv | Steps with order |
| interactions.csv | User engagement |

---

## **Load**
CSV files saved to:
data/csv_output/

These serve as the source for analytics.

---

#  4. Insights Summary

The analytics script produces at least **10 insights**, including:

### ðŸ”¹ 1. Most common ingredients  
(e.g., salt, bread, butter)

### ðŸ”¹ 2. Average preparation time  
(Example: 5.5 minutes)

### ðŸ”¹ 3. Difficulty distribution  
- easy: X  
- medium: Y  
- hard: Z  

### ðŸ”¹ 4. Correlation between prep time & likes  
Indicates if longer recipes get more likes.

### ðŸ”¹ 5. Most viewed recipes  
Top-viewed recipe IDs and names.

### ðŸ”¹ 6. High-engagement ingredients  
Which ingredients appear in popular recipes.

### ðŸ”¹ 7. Average number of ingredients per recipe  

### ðŸ”¹ 8. Most active users  

### ðŸ”¹ 9. Recipes with the highest ratings  

### ðŸ”¹ 10. Average steps count per recipe  

All insights saved to:
analytics_report.txt

---

#  5. Known Constraints / Limitations

- Firestore export requires correct serviceAccountKey.json  
- Small dataset â†’ not statistically robust  
- No automatic scheduling (manual run only)  
- ETL assumes every recipe contains valid structure  
- analytics.py uses descriptive statistics only  
- Recipe interactions are simplistic (view/like/rating)  
- Data quality is dependent on Firestore source  
- No cloud-based orchestration (e.g., Airflow, Cloud Composer)  
- No incremental ETL (full extract each run)

---

#  Final Notes

This project includes:

âœ” Firestore export  
âœ” ETL pipeline  
âœ” Data quality validation  
âœ” Analytics & insights  
âœ” Professional documentation (this README)

It demonstrates real-world data engineering principles including extraction, cleaning, normalization, validation, and reporting.

Feel free to extend the pipeline with visualization, scheduling, or dashboards.






